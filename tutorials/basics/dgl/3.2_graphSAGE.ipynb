{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# GraphSAGE\n",
    "This notebook demonstrates the training of [GraphSAGE models](https://arxiv.org/abs/1706.02216) with TigerGraph. [DGL](https://www.dgl.ai/)'s implementation of GraphSAGE is used here. We train the model on the Cora dataset from [PyG datasets](https://pytorch-geometric.readthedocs.io/en/latest/modules/datasets.html#torch_geometric.datasets.Planetoid) with TigerGraph as the data store. The dataset contains 2708 machine learning papers and 10556 citation links between the papers.  Each publication in the dataset is described by a 0/1-valued word vector indicating the absence/presence of the corresponding word from a dictionary. The dictionary consists of 1433 unique words. Each paper is classified into one of seven classes based on the topic. The goal is to predict the class of each vertex in the graph."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Table of Contents\n",
    "* [Data Processing](#data_processing)  \n",
    "* [Train on whole graph](#train_whole)  \n",
    "* [Train on neighborhood subgraphs](#train_subgraph)  "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data Processing <a name=\"data_processing\"></a>\n",
    "\n",
    "Here we assume the dataset is already ingested into the TigerGraph database. If not, please refer to the example on data ingestion first. Since the dataset already has a split of vertices into train/validation/test sets, we don't need to do so. But we still include the code below for general use cases."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Connect to TigerGraph"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "from tgml.data import TigerGraph\n",
    "\n",
    "tgraph = TigerGraph(\n",
    "    host=\"http://35.230.92.92\",\n",
    "    graph=\"Cora\",\n",
    "    username=\"tigergraph\",\n",
    "    password=\"tigergraphml\",\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "tgraph.info()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Using graph 'Cora'\n",
      "---- Graph Cora\n",
      "Vertex Types: \n",
      "  - VERTEX Paper(PRIMARY_ID id INT, x LIST<INT>, y INT, train_mask BOOL, val_mask BOOL, test_mask BOOL, tmp_id INT, tmp_id2 INT, tmp_id3 INT) WITH STATS=\"OUTDEGREE_BY_EDGETYPE\", PRIMARY_ID_AS_ATTRIBUTE=\"true\"\n",
      "Edge Types: \n",
      "  - DIRECTED EDGE Cite(FROM Paper, TO Paper)\n",
      "\n",
      "Graphs: \n",
      "  - Graph Cora(Paper:v, Cite:e)\n",
      "Jobs: \n",
      "Queries: \n",
      "  - export_edge(string output_path) (installed v2)\n",
      "  - export_edge_batch(string output_path, int batch_id, int num_batches) (installed v2)\n",
      "  - export_vertex_(string output_path) (installed v2)\n",
      "  - export_vertex_batch_x_y(string output_path, int batch_id, int num_batches) (installed v2)\n",
      "  - export_vertex_train_mask_val_mask_test_mask(string output_path) (installed v2)\n",
      "  - export_vertex_x_y(string output_path) (installed v2)\n",
      "  - export_vertex_x_y_train_mask_val_mask_test_mask(string output_path) (installed v2)\n",
      "  - get_vertex_number(string v_type, string filter_by) (installed v2)\n",
      "  - shuffle_vertices(string tmp_id) (installed v2)\n",
      "  - tg_neighbor_sampler_x_y_train_mask_val_mask_test_mask(string vertex_filename, string edge_filename, int batch_id, int num_batches, int num_neighbors, int num_hops, string filter_by, string tmp_id) (installed v2)\n",
      "  - tg_pagerank(string v_type, string e_type, float max_change, int max_iter, float damping, int top_k, bool print_accum, string result_attr, string file_path, bool display_edges) (installed v2)\n",
      "  - train_test_vertex_split(string train_attr, string test_attr, string val_attr, double train_ratio, double test_ratio) (installed v2)\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "tgraph.number_of_vertices()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "2708"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "tgraph.number_of_edges()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "10556"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Train/validation/test split"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "# The code in this cell is commented out because there is no need to split the vertices into training/validation/test sets, as the split is already done in the original dataset. See notebook 1_data_processing for examples on the split function.\n",
    "\n",
    "# from tgml.utils import split_vertices\n",
    "# split_vertices(tgraph, train_mask=0.8, val_mask=0.1, test_mask=0.1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "print(\n",
    "    \"Number of vertices in training set:\",\n",
    "    tgraph.number_of_vertices(filter_by=\"train_mask\"),\n",
    ")\n",
    "print(\n",
    "    \"Number of vertices in validation set:\",\n",
    "    tgraph.number_of_vertices(filter_by=\"val_mask\"),\n",
    ")\n",
    "print(\n",
    "    \"Number of vertices in test set:\", tgraph.number_of_vertices(filter_by=\"test_mask\")\n",
    ")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Number of vertices in training set: 2183\n",
      "Number of vertices in validation set: 273\n",
      "Number of vertices in test set: 252\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Train on whole graph <a name=\"train_whole\"></a>\n",
    "We first train the model on the whole graph. This works when the graph is small, but it is not efficient or even not possible when the graph is large. Hyperparameters for the model and training environment are defined below."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "# Hyperparameters\n",
    "hp = {\"hidden_dim\": 64, \n",
    "      \"num_layers\": 2, \n",
    "      \"dropout\": 0.6, \n",
    "      \"lr\": 0.01, \n",
    "      \"l2_penalty\": 5e-4}"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Construct graph loader"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The `GraphLoader` will get the whole graph from database all at once. (See the tutorial on dataloaders for details.) "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "from tgml.dataloaders import GraphLoader\n",
    "\n",
    "graph_loader = GraphLoader(\n",
    "    graph=tgraph,\n",
    "    v_in_feats=\"x\",\n",
    "    v_out_labels=\"y:int\",\n",
    "    v_extra_feats=\"train_mask:bool,val_mask:bool,test_mask:bool\",\n",
    "    output_format=\"DGL\",\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "# Get the whole graph from the loader in DGL format\n",
    "data = graph_loader.data\n",
    "\n",
    "data"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Using backend: pytorch\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Graph(num_nodes=2708, num_edges=10556,\n",
       "      ndata_schemes={'feat': Scheme(shape=(1433,), dtype=torch.float32), 'label': Scheme(shape=(), dtype=torch.int64), 'train_mask': Scheme(shape=(), dtype=torch.bool), 'val_mask': Scheme(shape=(), dtype=torch.bool), 'test_mask': Scheme(shape=(), dtype=torch.bool)}\n",
       "      edata_schemes={})"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Construct model and optimizer"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We build a graphSAGE model with 2 convolutional layers, and use the Adam optimizer with a learning rate of 0.01."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "import dgl.function as fn\n",
    "import dgl.nn.pytorch as dglnn\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "class GraphSage(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GraphSage, self).__init__()\n",
    "        self.layer1 = dglnn.conv.SAGEConv(1433, hp['hidden_dim'], aggregator_type=\"mean\")\n",
    "        self.layer2 = dglnn.conv.SAGEConv(hp['hidden_dim'], 7, aggregator_type=\"mean\")\n",
    "\n",
    "    def forward(self, g, features):\n",
    "        x = F.relu(self.layer1(g, features))\n",
    "        x = self.layer2(g, x)\n",
    "        return x\n",
    "\n",
    "model = GraphSage()\n",
    "print(model)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "GraphSage(\n",
      "  (layer1): SAGEConv(\n",
      "    (feat_drop): Dropout(p=0.0, inplace=False)\n",
      "    (fc_self): Linear(in_features=1433, out_features=64, bias=False)\n",
      "    (fc_neigh): Linear(in_features=1433, out_features=64, bias=False)\n",
      "  )\n",
      "  (layer2): SAGEConv(\n",
      "    (feat_drop): Dropout(p=0.0, inplace=False)\n",
      "    (fc_self): Linear(in_features=64, out_features=7, bias=False)\n",
      "    (fc_neigh): Linear(in_features=64, out_features=7, bias=False)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(), lr=hp[\"lr\"], weight_decay=hp[\"l2_penalty\"]\n",
    ")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "from datetime import datetime\n",
    "\n",
    "from tgml.metrics import Accuracy\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "log_dir = \"logs/cora/gcn/wholegraph/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tb_log = SummaryWriter(log_dir)\n",
    "logs = {}\n",
    "data = data.to(device)\n",
    "for epoch in range(20):\n",
    "    # Train\n",
    "    model.train()\n",
    "    acc = Accuracy()\n",
    "    # Forward pass\n",
    "    out = model(data, data.ndata[\"feat\"])\n",
    "    # Calculate loss\n",
    "    loss = F.cross_entropy(out[data.ndata[\"train_mask\"]], data.ndata[\"label\"][data.ndata[\"train_mask\"]])\n",
    "    # Backward pass\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    # Evaluate\n",
    "    val_acc = Accuracy()\n",
    "    with torch.no_grad():\n",
    "        pred = out.argmax(dim=1)\n",
    "        acc.update(pred[data.ndata[\"train_mask\"]], data.ndata[\"label\"][data.ndata[\"train_mask\"]])\n",
    "        valid_loss = F.cross_entropy(out[data.ndata[\"val_mask\"]], data.ndata[\"label\"][data.ndata[\"val_mask\"]])\n",
    "        val_acc.update(pred[data.ndata[\"val_mask\"]], data.ndata[\"label\"][data.ndata[\"val_mask\"]])\n",
    "    # Logging\n",
    "    logs[\"loss\"] = loss.item()\n",
    "    logs[\"val_loss\"] = valid_loss.item()\n",
    "    logs[\"acc\"] = acc.value\n",
    "    logs[\"val_acc\"] = val_acc.value\n",
    "    print(\n",
    "        \"Epoch: {:02d}, Train Loss: {:.4f}, Valid Loss: {:.4f}, Train Accuracy: {:.4f}, Valid Accuracy: {:.4f}\".format(\n",
    "            epoch, logs[\"loss\"], logs[\"val_loss\"], logs[\"acc\"], logs[\"val_acc\"]\n",
    "        )\n",
    "    )\n",
    "    tb_log.add_scalars(\n",
    "        \"Loss\", {\"Train\": logs[\"loss\"], \"Validation\": logs[\"val_loss\"]}, epoch\n",
    "    )\n",
    "    tb_log.add_scalars(\n",
    "        \"Accuracy\", {\"Train\": logs[\"acc\"], \"Validation\": logs[\"val_acc\"]}, epoch\n",
    "    )\n",
    "    tb_log.flush()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch: 00, Train Loss: 1.9294, Valid Loss: 1.8947, Train Accuracy: 0.2071, Valid Accuracy: 0.2308\n",
      "Epoch: 01, Train Loss: 1.2665, Valid Loss: 1.3486, Train Accuracy: 0.4622, Valid Accuracy: 0.4286\n",
      "Epoch: 02, Train Loss: 0.7014, Valid Loss: 0.8547, Train Accuracy: 0.8475, Valid Accuracy: 0.7912\n",
      "Epoch: 03, Train Loss: 0.4334, Valid Loss: 0.6164, Train Accuracy: 0.9116, Valid Accuracy: 0.8498\n",
      "Epoch: 04, Train Loss: 0.2869, Valid Loss: 0.4978, Train Accuracy: 0.9226, Valid Accuracy: 0.8425\n",
      "Epoch: 05, Train Loss: 0.1998, Valid Loss: 0.4245, Train Accuracy: 0.9441, Valid Accuracy: 0.8681\n",
      "Epoch: 06, Train Loss: 0.1556, Valid Loss: 0.3970, Train Accuracy: 0.9501, Valid Accuracy: 0.8791\n",
      "Epoch: 07, Train Loss: 0.1255, Valid Loss: 0.3935, Train Accuracy: 0.9620, Valid Accuracy: 0.8755\n",
      "Epoch: 08, Train Loss: 0.0993, Valid Loss: 0.3972, Train Accuracy: 0.9734, Valid Accuracy: 0.8828\n",
      "Epoch: 09, Train Loss: 0.0766, Valid Loss: 0.4012, Train Accuracy: 0.9794, Valid Accuracy: 0.8864\n",
      "Epoch: 10, Train Loss: 0.0578, Valid Loss: 0.4054, Train Accuracy: 0.9867, Valid Accuracy: 0.8864\n",
      "Epoch: 11, Train Loss: 0.0440, Valid Loss: 0.4130, Train Accuracy: 0.9918, Valid Accuracy: 0.8864\n",
      "Epoch: 12, Train Loss: 0.0347, Valid Loss: 0.4265, Train Accuracy: 0.9931, Valid Accuracy: 0.8901\n",
      "Epoch: 13, Train Loss: 0.0286, Valid Loss: 0.4428, Train Accuracy: 0.9940, Valid Accuracy: 0.8901\n",
      "Epoch: 14, Train Loss: 0.0237, Valid Loss: 0.4554, Train Accuracy: 0.9950, Valid Accuracy: 0.8938\n",
      "Epoch: 15, Train Loss: 0.0192, Valid Loss: 0.4608, Train Accuracy: 0.9968, Valid Accuracy: 0.8974\n",
      "Epoch: 16, Train Loss: 0.0151, Valid Loss: 0.4595, Train Accuracy: 0.9982, Valid Accuracy: 0.8938\n",
      "Epoch: 17, Train Loss: 0.0120, Valid Loss: 0.4552, Train Accuracy: 0.9986, Valid Accuracy: 0.8974\n",
      "Epoch: 18, Train Loss: 0.0098, Valid Loss: 0.4511, Train Accuracy: 0.9995, Valid Accuracy: 0.8974\n",
      "Epoch: 19, Train Loss: 0.0084, Valid Loss: 0.4488, Train Accuracy: 0.9995, Valid Accuracy: 0.8938\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "model.eval()\n",
    "acc = Accuracy()\n",
    "with torch.no_grad():\n",
    "    pred = model(data, data.ndata[\"feat\"]).argmax(dim=1)\n",
    "    acc.update(pred[data.ndata[\"test_mask\"]], data.ndata[\"label\"][data.ndata[\"test_mask\"]])\n",
    "print(\"Accuracy: {:.4f}\".format(acc.value))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Accuracy: 0.8810\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Train on Neighborhood Subgraphs <a name=\"train_subgraph\"></a>\n",
    "Alternatively, we train the model on the neighborhood subgraphs. Each subgraph contains the 2 hop neighborhood of certain seed vertices. This method  will allow us to train the model on graphs that are way larger than the CORA dataset because we don't load the whole graph into memory all at once. \n",
    "\n",
    "We will use the same parameters as before, but we will use the NeighborLoader to load subgraphs. Once we finish iterating over all the subgraphs generated by the loader, it is guaranteed to cover all vertices in the graph (except for those filtered by a user provided mask). "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "# Hyperparameters\n",
    "hp = {\"batch_size\": 64, \n",
    "      \"num_neighbors\": 10, \n",
    "      \"num_hops\": 2, \n",
    "      \"hidden_dim\": 64, \n",
    "      \"num_layers\": 2, \n",
    "      \"dropout\": 0.6, \n",
    "      \"lr\": 0.01, \n",
    "      \"l2_penalty\": 5e-4}"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Construct neighborhood subgraph loader"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "from tgml.dataloaders import NeighborLoader"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here we construct 3 subgraph loaders. The `train_loader` only uses vertices in the training set as seeds, the `valid_loader` only uses vertices in the validation set, and the `test_loader` only uses vertices in the test set."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "train_loader = NeighborLoader(\n",
    "    graph=tgraph,\n",
    "    tmp_id=\"tmp_id\",\n",
    "    v_in_feats=\"x\",\n",
    "    v_out_labels=\"y:int\",\n",
    "    v_extra_feats=\"train_mask:bool,val_mask:bool,test_mask:bool\",\n",
    "    output_format=\"DGL\",\n",
    "    batch_size=hp[\"batch_size\"],\n",
    "    num_neighbors=hp[\"num_neighbors\"],\n",
    "    num_hops=hp[\"num_hops\"],\n",
    "    shuffle=True,\n",
    "    filter_by=\"train_mask\",\n",
    "    add_self_loop=True\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "valid_loader = NeighborLoader(\n",
    "    graph=tgraph,\n",
    "    tmp_id=\"tmp_id2\",\n",
    "    v_in_feats=\"x\",\n",
    "    v_out_labels=\"y:int\",\n",
    "    v_extra_feats=\"train_mask:bool,val_mask:bool,test_mask:bool\",\n",
    "    output_format=\"DGL\",\n",
    "    batch_size=hp[\"batch_size\"],\n",
    "    num_neighbors=hp[\"num_neighbors\"],\n",
    "    num_hops=hp[\"num_hops\"],\n",
    "    shuffle=False,\n",
    "    filter_by=\"val_mask\",\n",
    "    add_self_loop=True\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "test_loader = NeighborLoader(\n",
    "    graph=tgraph,\n",
    "    tmp_id=\"tmp_id3\",\n",
    "    v_in_feats=\"x\",\n",
    "    v_out_labels=\"y:int\",\n",
    "    v_extra_feats=\"train_mask:bool,val_mask:bool,test_mask:bool\",\n",
    "    output_format=\"DGL\",\n",
    "    batch_size=hp[\"batch_size\"],\n",
    "    num_neighbors=hp[\"num_neighbors\"],\n",
    "    num_hops=hp[\"num_hops\"],\n",
    "    shuffle=False,\n",
    "    filter_by=\"test_mask\",\n",
    "    add_self_loop=True\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Construct model and optimizer\n",
    "We build a GCN model with 2 convolutional layers, and use the Adam optimizer with a learning rate of 0.01."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = GraphSage().to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(), lr=hp[\"lr\"], weight_decay=hp[\"l2_penalty\"]\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Train the model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "source": [
    "from datetime import datetime\n",
    "\n",
    "from tgml.metrics import Accumulator, Accuracy\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "source": [
    "log_dir = \"logs/cora/gcn/subgraph/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "train_log = SummaryWriter(log_dir+\"/train\")\n",
    "valid_log = SummaryWriter(log_dir+\"/valid\")\n",
    "global_steps = 0\n",
    "logs = {}\n",
    "for epoch in range(10):\n",
    "    # Train\n",
    "    model.train()\n",
    "    epoch_train_loss = Accumulator()\n",
    "    epoch_train_acc = Accuracy()\n",
    "    for bid, batch in enumerate(train_loader):\n",
    "        batchsize = batch.num_nodes()\n",
    "        batch.to(device)\n",
    "        # Forward pass\n",
    "        out = model(batch, batch.ndata[\"feat\"])\n",
    "        # Calculate loss\n",
    "        loss = F.cross_entropy(out[batch.ndata[\"train_mask\"]], batch.ndata[\"label\"][batch.ndata[\"train_mask\"]])\n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_train_loss.update(loss.item() * batchsize, batchsize)\n",
    "        # Predict on training data\n",
    "        with torch.no_grad():\n",
    "            pred = out.argmax(dim=1)\n",
    "            epoch_train_acc.update(pred[batch.ndata[\"train_mask\"]], batch.ndata[\"label\"][batch.ndata[\"train_mask\"]])\n",
    "        # Log training status after each batch\n",
    "        logs[\"loss\"] = epoch_train_loss.mean\n",
    "        logs[\"acc\"] = epoch_train_acc.value\n",
    "        print(\n",
    "            \"Epoch {}, Train Batch {}, Loss {:.4f}, Accuracy {:.4f}\".format(\n",
    "                epoch, bid, logs[\"loss\"], logs[\"acc\"]\n",
    "            )\n",
    "        )\n",
    "        train_log.add_scalar(\"Loss\", logs[\"loss\"], global_steps)\n",
    "        train_log.add_scalar(\"Accuracy\", logs[\"acc\"], global_steps)\n",
    "        train_log.flush()\n",
    "        global_steps += 1\n",
    "    # Evaluate\n",
    "    model.eval()\n",
    "    epoch_val_loss = Accumulator()\n",
    "    epoch_val_acc = Accuracy()\n",
    "    for batch in valid_loader:\n",
    "        batchsize = batch.num_nodes()\n",
    "        batch.to(device)\n",
    "        with torch.no_grad():\n",
    "            # Forward pass\n",
    "            out = model(batch, batch.ndata[\"feat\"])\n",
    "            # Calculate loss\n",
    "            valid_loss = F.cross_entropy(out[batch.ndata[\"val_mask\"]], batch.ndata[\"label\"][batch.ndata[\"val_mask\"]])\n",
    "            epoch_val_loss.update(valid_loss.item() * batchsize, batchsize)\n",
    "            # Prediction\n",
    "            pred = out.argmax(dim=1)\n",
    "            epoch_val_acc.update(pred[batch.ndata[\"val_mask\"]], batch.ndata[\"label\"][batch.ndata[\"val_mask\"]])\n",
    "    # Log testing result after each epoch\n",
    "    logs[\"val_loss\"] = epoch_val_loss.mean\n",
    "    logs[\"val_acc\"] = epoch_val_acc.value\n",
    "    print(\n",
    "        \"Epoch {}, Valid Loss {:.4f}, Valid Accuracy {:.4f}\".format(\n",
    "            epoch, logs[\"val_loss\"], logs[\"val_acc\"]\n",
    "        )\n",
    "    )\n",
    "    valid_log.add_scalar(\"Loss\", logs[\"val_loss\"], global_steps)\n",
    "    valid_log.add_scalar(\"Accuracy\", logs[\"val_acc\"], global_steps)\n",
    "    valid_log.flush()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 0, Train Batch 0, Loss 2.1366, Accuracy 0.1272\n",
      "Epoch 0, Train Batch 1, Loss 1.7572, Accuracy 0.3767\n",
      "Epoch 0, Train Batch 2, Loss 1.5762, Accuracy 0.4620\n",
      "Epoch 0, Train Batch 3, Loss 1.3986, Accuracy 0.5150\n",
      "Epoch 0, Train Batch 4, Loss 1.3005, Accuracy 0.5572\n",
      "Epoch 0, Train Batch 5, Loss 1.1661, Accuracy 0.6097\n",
      "Epoch 0, Train Batch 6, Loss 1.0671, Accuracy 0.6455\n",
      "Epoch 0, Train Batch 7, Loss 0.9866, Accuracy 0.6729\n",
      "Epoch 0, Train Batch 8, Loss 0.9094, Accuracy 0.7003\n",
      "Epoch 0, Train Batch 9, Loss 0.8649, Accuracy 0.7158\n",
      "Epoch 0, Train Batch 10, Loss 0.8127, Accuracy 0.7334\n",
      "Epoch 0, Train Batch 11, Loss 0.7643, Accuracy 0.7504\n",
      "Epoch 0, Train Batch 12, Loss 0.7239, Accuracy 0.7633\n",
      "Epoch 0, Train Batch 13, Loss 0.6920, Accuracy 0.7748\n",
      "Epoch 0, Train Batch 14, Loss 0.6677, Accuracy 0.7823\n",
      "Epoch 0, Train Batch 15, Loss 0.6400, Accuracy 0.7915\n",
      "Epoch 0, Train Batch 16, Loss 0.6115, Accuracy 0.8003\n",
      "Epoch 0, Train Batch 17, Loss 0.5896, Accuracy 0.8068\n",
      "Epoch 0, Train Batch 18, Loss 0.5689, Accuracy 0.8132\n",
      "Epoch 0, Train Batch 19, Loss 0.5451, Accuracy 0.8209\n",
      "Epoch 0, Train Batch 20, Loss 0.5288, Accuracy 0.8262\n",
      "Epoch 0, Train Batch 21, Loss 0.5086, Accuracy 0.8329\n",
      "Epoch 0, Train Batch 22, Loss 0.4913, Accuracy 0.8389\n",
      "Epoch 0, Train Batch 23, Loss 0.4763, Accuracy 0.8438\n",
      "Epoch 0, Train Batch 24, Loss 0.4592, Accuracy 0.8498\n",
      "Epoch 0, Train Batch 25, Loss 0.4430, Accuracy 0.8553\n",
      "Epoch 0, Train Batch 26, Loss 0.4322, Accuracy 0.8590\n",
      "Epoch 0, Train Batch 27, Loss 0.4205, Accuracy 0.8628\n",
      "Epoch 0, Train Batch 28, Loss 0.4109, Accuracy 0.8661\n",
      "Epoch 0, Train Batch 29, Loss 0.3994, Accuracy 0.8700\n",
      "Epoch 0, Train Batch 30, Loss 0.3901, Accuracy 0.8729\n",
      "Epoch 0, Train Batch 31, Loss 0.3820, Accuracy 0.8756\n",
      "Epoch 0, Train Batch 32, Loss 0.3729, Accuracy 0.8785\n",
      "Epoch 0, Train Batch 33, Loss 0.3631, Accuracy 0.8820\n",
      "Epoch 0, Train Batch 34, Loss 0.3538, Accuracy 0.8850\n",
      "Epoch 0, Valid Loss 0.4461, Valid Accuracy 0.8669\n",
      "Epoch 1, Train Batch 0, Loss 0.0617, Accuracy 0.9876\n",
      "Epoch 1, Train Batch 1, Loss 0.0545, Accuracy 0.9865\n",
      "Epoch 1, Train Batch 2, Loss 0.0490, Accuracy 0.9889\n",
      "Epoch 1, Train Batch 3, Loss 0.0473, Accuracy 0.9897\n",
      "Epoch 1, Train Batch 4, Loss 0.0465, Accuracy 0.9893\n",
      "Epoch 1, Train Batch 5, Loss 0.0441, Accuracy 0.9899\n",
      "Epoch 1, Train Batch 6, Loss 0.0435, Accuracy 0.9899\n",
      "Epoch 1, Train Batch 7, Loss 0.0426, Accuracy 0.9905\n",
      "Epoch 1, Train Batch 8, Loss 0.0430, Accuracy 0.9898\n",
      "Epoch 1, Train Batch 9, Loss 0.0423, Accuracy 0.9898\n",
      "Epoch 1, Train Batch 10, Loss 0.0411, Accuracy 0.9901\n",
      "Epoch 1, Train Batch 11, Loss 0.0414, Accuracy 0.9899\n",
      "Epoch 1, Train Batch 12, Loss 0.0401, Accuracy 0.9900\n",
      "Epoch 1, Train Batch 13, Loss 0.0393, Accuracy 0.9903\n",
      "Epoch 1, Train Batch 14, Loss 0.0387, Accuracy 0.9906\n",
      "Epoch 1, Train Batch 15, Loss 0.0381, Accuracy 0.9908\n",
      "Epoch 1, Train Batch 16, Loss 0.0367, Accuracy 0.9914\n",
      "Epoch 1, Train Batch 17, Loss 0.0358, Accuracy 0.9917\n",
      "Epoch 1, Train Batch 18, Loss 0.0352, Accuracy 0.9920\n",
      "Epoch 1, Train Batch 19, Loss 0.0350, Accuracy 0.9923\n",
      "Epoch 1, Train Batch 20, Loss 0.0343, Accuracy 0.9925\n",
      "Epoch 1, Train Batch 21, Loss 0.0344, Accuracy 0.9924\n",
      "Epoch 1, Train Batch 22, Loss 0.0338, Accuracy 0.9927\n",
      "Epoch 1, Train Batch 23, Loss 0.0332, Accuracy 0.9929\n",
      "Epoch 1, Train Batch 24, Loss 0.0325, Accuracy 0.9931\n",
      "Epoch 1, Train Batch 25, Loss 0.0322, Accuracy 0.9934\n",
      "Epoch 1, Train Batch 26, Loss 0.0318, Accuracy 0.9935\n",
      "Epoch 1, Train Batch 27, Loss 0.0317, Accuracy 0.9935\n",
      "Epoch 1, Train Batch 28, Loss 0.0317, Accuracy 0.9936\n",
      "Epoch 1, Train Batch 29, Loss 0.0317, Accuracy 0.9937\n",
      "Epoch 1, Train Batch 30, Loss 0.0317, Accuracy 0.9938\n",
      "Epoch 1, Train Batch 31, Loss 0.0317, Accuracy 0.9939\n",
      "Epoch 1, Train Batch 32, Loss 0.0314, Accuracy 0.9941\n",
      "Epoch 1, Train Batch 33, Loss 0.0316, Accuracy 0.9940\n",
      "Epoch 1, Train Batch 34, Loss 0.0313, Accuracy 0.9941\n",
      "Epoch 1, Valid Loss 0.4310, Valid Accuracy 0.8789\n",
      "Epoch 2, Train Batch 0, Loss 0.0174, Accuracy 0.9979\n",
      "Epoch 2, Train Batch 1, Loss 0.0197, Accuracy 0.9977\n",
      "Epoch 2, Train Batch 2, Loss 0.0192, Accuracy 0.9980\n",
      "Epoch 2, Train Batch 3, Loss 0.0182, Accuracy 0.9985\n",
      "Epoch 2, Train Batch 4, Loss 0.0187, Accuracy 0.9985\n",
      "Epoch 2, Train Batch 5, Loss 0.0185, Accuracy 0.9988\n",
      "Epoch 2, Train Batch 6, Loss 0.0184, Accuracy 0.9987\n",
      "Epoch 2, Train Batch 7, Loss 0.0184, Accuracy 0.9989\n",
      "Epoch 2, Train Batch 8, Loss 0.0182, Accuracy 0.9989\n",
      "Epoch 2, Train Batch 9, Loss 0.0185, Accuracy 0.9990\n",
      "Epoch 2, Train Batch 10, Loss 0.0188, Accuracy 0.9991\n",
      "Epoch 2, Train Batch 11, Loss 0.0195, Accuracy 0.9989\n",
      "Epoch 2, Train Batch 12, Loss 0.0197, Accuracy 0.9986\n",
      "Epoch 2, Train Batch 13, Loss 0.0203, Accuracy 0.9985\n",
      "Epoch 2, Train Batch 14, Loss 0.0199, Accuracy 0.9987\n",
      "Epoch 2, Train Batch 15, Loss 0.0198, Accuracy 0.9987\n",
      "Epoch 2, Train Batch 16, Loss 0.0197, Accuracy 0.9988\n",
      "Epoch 2, Train Batch 17, Loss 0.0196, Accuracy 0.9989\n",
      "Epoch 2, Train Batch 18, Loss 0.0196, Accuracy 0.9987\n",
      "Epoch 2, Train Batch 19, Loss 0.0195, Accuracy 0.9988\n",
      "Epoch 2, Train Batch 20, Loss 0.0196, Accuracy 0.9989\n",
      "Epoch 2, Train Batch 21, Loss 0.0198, Accuracy 0.9988\n",
      "Epoch 2, Train Batch 22, Loss 0.0200, Accuracy 0.9987\n",
      "Epoch 2, Train Batch 23, Loss 0.0201, Accuracy 0.9986\n",
      "Epoch 2, Train Batch 24, Loss 0.0203, Accuracy 0.9986\n",
      "Epoch 2, Train Batch 25, Loss 0.0205, Accuracy 0.9985\n",
      "Epoch 2, Train Batch 26, Loss 0.0207, Accuracy 0.9985\n",
      "Epoch 2, Train Batch 27, Loss 0.0209, Accuracy 0.9984\n",
      "Epoch 2, Train Batch 28, Loss 0.0208, Accuracy 0.9985\n",
      "Epoch 2, Train Batch 29, Loss 0.0208, Accuracy 0.9985\n",
      "Epoch 2, Train Batch 30, Loss 0.0209, Accuracy 0.9985\n",
      "Epoch 2, Train Batch 31, Loss 0.0209, Accuracy 0.9985\n",
      "Epoch 2, Train Batch 32, Loss 0.0211, Accuracy 0.9984\n",
      "Epoch 2, Train Batch 33, Loss 0.0212, Accuracy 0.9984\n",
      "Epoch 2, Train Batch 34, Loss 0.0212, Accuracy 0.9984\n",
      "Epoch 2, Valid Loss 0.4364, Valid Accuracy 0.8619\n",
      "Epoch 3, Train Batch 0, Loss 0.0177, Accuracy 1.0000\n",
      "Epoch 3, Train Batch 1, Loss 0.0179, Accuracy 0.9994\n",
      "Epoch 3, Train Batch 2, Loss 0.0187, Accuracy 0.9988\n",
      "Epoch 3, Train Batch 3, Loss 0.0192, Accuracy 0.9988\n",
      "Epoch 3, Train Batch 4, Loss 0.0200, Accuracy 0.9987\n",
      "Epoch 3, Train Batch 5, Loss 0.0195, Accuracy 0.9990\n",
      "Epoch 3, Train Batch 6, Loss 0.0197, Accuracy 0.9991\n",
      "Epoch 3, Train Batch 7, Loss 0.0194, Accuracy 0.9992\n",
      "Epoch 3, Train Batch 8, Loss 0.0194, Accuracy 0.9990\n",
      "Epoch 3, Train Batch 9, Loss 0.0197, Accuracy 0.9989\n",
      "Epoch 3, Train Batch 10, Loss 0.0194, Accuracy 0.9988\n",
      "Epoch 3, Train Batch 11, Loss 0.0195, Accuracy 0.9986\n",
      "Epoch 3, Train Batch 12, Loss 0.0197, Accuracy 0.9986\n",
      "Epoch 3, Train Batch 13, Loss 0.0195, Accuracy 0.9987\n",
      "Epoch 3, Train Batch 14, Loss 0.0198, Accuracy 0.9987\n",
      "Epoch 3, Train Batch 15, Loss 0.0199, Accuracy 0.9987\n",
      "Epoch 3, Train Batch 16, Loss 0.0200, Accuracy 0.9987\n",
      "Epoch 3, Train Batch 17, Loss 0.0200, Accuracy 0.9987\n",
      "Epoch 3, Train Batch 18, Loss 0.0201, Accuracy 0.9987\n",
      "Epoch 3, Train Batch 19, Loss 0.0200, Accuracy 0.9987\n",
      "Epoch 3, Train Batch 20, Loss 0.0198, Accuracy 0.9988\n",
      "Epoch 3, Train Batch 21, Loss 0.0200, Accuracy 0.9987\n",
      "Epoch 3, Train Batch 22, Loss 0.0199, Accuracy 0.9988\n",
      "Epoch 3, Train Batch 23, Loss 0.0199, Accuracy 0.9988\n",
      "Epoch 3, Train Batch 24, Loss 0.0200, Accuracy 0.9987\n",
      "Epoch 3, Train Batch 25, Loss 0.0201, Accuracy 0.9987\n",
      "Epoch 3, Train Batch 26, Loss 0.0203, Accuracy 0.9986\n",
      "Epoch 3, Train Batch 27, Loss 0.0202, Accuracy 0.9986\n",
      "Epoch 3, Train Batch 28, Loss 0.0202, Accuracy 0.9986\n",
      "Epoch 3, Train Batch 29, Loss 0.0202, Accuracy 0.9987\n",
      "Epoch 3, Train Batch 30, Loss 0.0203, Accuracy 0.9987\n",
      "Epoch 3, Train Batch 31, Loss 0.0202, Accuracy 0.9987\n",
      "Epoch 3, Train Batch 32, Loss 0.0206, Accuracy 0.9986\n",
      "Epoch 3, Train Batch 33, Loss 0.0206, Accuracy 0.9986\n",
      "Epoch 3, Train Batch 34, Loss 0.0207, Accuracy 0.9986\n",
      "Epoch 3, Valid Loss 0.4583, Valid Accuracy 0.8599\n",
      "Epoch 4, Train Batch 0, Loss 0.0156, Accuracy 1.0000\n",
      "Epoch 4, Train Batch 1, Loss 0.0178, Accuracy 0.9994\n",
      "Epoch 4, Train Batch 2, Loss 0.0163, Accuracy 0.9995\n",
      "Epoch 4, Train Batch 3, Loss 0.0172, Accuracy 0.9996\n",
      "Epoch 4, Train Batch 4, Loss 0.0173, Accuracy 0.9992\n",
      "Epoch 4, Train Batch 5, Loss 0.0171, Accuracy 0.9993\n",
      "Epoch 4, Train Batch 6, Loss 0.0178, Accuracy 0.9992\n",
      "Epoch 4, Train Batch 7, Loss 0.0184, Accuracy 0.9990\n",
      "Epoch 4, Train Batch 8, Loss 0.0181, Accuracy 0.9991\n",
      "Epoch 4, Train Batch 9, Loss 0.0189, Accuracy 0.9987\n",
      "Epoch 4, Train Batch 10, Loss 0.0186, Accuracy 0.9988\n",
      "Epoch 4, Train Batch 11, Loss 0.0185, Accuracy 0.9988\n",
      "Epoch 4, Train Batch 12, Loss 0.0182, Accuracy 0.9988\n",
      "Epoch 4, Train Batch 13, Loss 0.0181, Accuracy 0.9989\n",
      "Epoch 4, Train Batch 14, Loss 0.0178, Accuracy 0.9990\n",
      "Epoch 4, Train Batch 15, Loss 0.0178, Accuracy 0.9989\n",
      "Epoch 4, Train Batch 16, Loss 0.0179, Accuracy 0.9988\n",
      "Epoch 4, Train Batch 17, Loss 0.0180, Accuracy 0.9988\n",
      "Epoch 4, Train Batch 18, Loss 0.0179, Accuracy 0.9989\n",
      "Epoch 4, Train Batch 19, Loss 0.0181, Accuracy 0.9988\n",
      "Epoch 4, Train Batch 20, Loss 0.0183, Accuracy 0.9987\n",
      "Epoch 4, Train Batch 21, Loss 0.0185, Accuracy 0.9987\n",
      "Epoch 4, Train Batch 22, Loss 0.0190, Accuracy 0.9987\n",
      "Epoch 4, Train Batch 23, Loss 0.0189, Accuracy 0.9986\n",
      "Epoch 4, Train Batch 24, Loss 0.0187, Accuracy 0.9987\n",
      "Epoch 4, Train Batch 25, Loss 0.0189, Accuracy 0.9986\n",
      "Epoch 4, Train Batch 26, Loss 0.0190, Accuracy 0.9986\n",
      "Epoch 4, Train Batch 27, Loss 0.0189, Accuracy 0.9985\n",
      "Epoch 4, Train Batch 28, Loss 0.0188, Accuracy 0.9986\n",
      "Epoch 4, Train Batch 29, Loss 0.0190, Accuracy 0.9986\n",
      "Epoch 4, Train Batch 30, Loss 0.0190, Accuracy 0.9986\n",
      "Epoch 4, Train Batch 31, Loss 0.0190, Accuracy 0.9986\n",
      "Epoch 4, Train Batch 32, Loss 0.0189, Accuracy 0.9986\n",
      "Epoch 4, Train Batch 33, Loss 0.0193, Accuracy 0.9985\n",
      "Epoch 4, Train Batch 34, Loss 0.0195, Accuracy 0.9984\n",
      "Epoch 4, Valid Loss 0.4205, Valid Accuracy 0.8814\n",
      "Epoch 5, Train Batch 0, Loss 0.0182, Accuracy 0.9973\n",
      "Epoch 5, Train Batch 1, Loss 0.0186, Accuracy 0.9987\n",
      "Epoch 5, Train Batch 2, Loss 0.0171, Accuracy 0.9991\n",
      "Epoch 5, Train Batch 3, Loss 0.0178, Accuracy 0.9987\n",
      "Epoch 5, Train Batch 4, Loss 0.0167, Accuracy 0.9990\n",
      "Epoch 5, Train Batch 5, Loss 0.0165, Accuracy 0.9991\n",
      "Epoch 5, Train Batch 6, Loss 0.0184, Accuracy 0.9985\n",
      "Epoch 5, Train Batch 7, Loss 0.0189, Accuracy 0.9982\n",
      "Epoch 5, Train Batch 8, Loss 0.0188, Accuracy 0.9982\n",
      "Epoch 5, Train Batch 9, Loss 0.0185, Accuracy 0.9983\n",
      "Epoch 5, Train Batch 10, Loss 0.0186, Accuracy 0.9983\n",
      "Epoch 5, Train Batch 11, Loss 0.0186, Accuracy 0.9982\n",
      "Epoch 5, Train Batch 12, Loss 0.0190, Accuracy 0.9980\n",
      "Epoch 5, Train Batch 13, Loss 0.0189, Accuracy 0.9981\n",
      "Epoch 5, Train Batch 14, Loss 0.0188, Accuracy 0.9982\n",
      "Epoch 5, Train Batch 15, Loss 0.0186, Accuracy 0.9982\n",
      "Epoch 5, Train Batch 16, Loss 0.0188, Accuracy 0.9982\n",
      "Epoch 5, Train Batch 17, Loss 0.0192, Accuracy 0.9980\n",
      "Epoch 5, Train Batch 18, Loss 0.0191, Accuracy 0.9981\n",
      "Epoch 5, Train Batch 19, Loss 0.0192, Accuracy 0.9981\n",
      "Epoch 5, Train Batch 20, Loss 0.0193, Accuracy 0.9979\n",
      "Epoch 5, Train Batch 21, Loss 0.0196, Accuracy 0.9978\n",
      "Epoch 5, Train Batch 22, Loss 0.0196, Accuracy 0.9979\n",
      "Epoch 5, Train Batch 23, Loss 0.0197, Accuracy 0.9978\n",
      "Epoch 5, Train Batch 24, Loss 0.0198, Accuracy 0.9978\n",
      "Epoch 5, Train Batch 25, Loss 0.0198, Accuracy 0.9978\n",
      "Epoch 5, Train Batch 26, Loss 0.0199, Accuracy 0.9977\n",
      "Epoch 5, Train Batch 27, Loss 0.0202, Accuracy 0.9976\n",
      "Epoch 5, Train Batch 28, Loss 0.0206, Accuracy 0.9975\n",
      "Epoch 5, Train Batch 29, Loss 0.0207, Accuracy 0.9975\n",
      "Epoch 5, Train Batch 30, Loss 0.0209, Accuracy 0.9974\n",
      "Epoch 5, Train Batch 31, Loss 0.0210, Accuracy 0.9974\n",
      "Epoch 5, Train Batch 32, Loss 0.0212, Accuracy 0.9972\n",
      "Epoch 5, Train Batch 33, Loss 0.0210, Accuracy 0.9973\n",
      "Epoch 5, Train Batch 34, Loss 0.0214, Accuracy 0.9972\n",
      "Epoch 5, Valid Loss 0.4996, Valid Accuracy 0.8549\n",
      "Epoch 6, Train Batch 0, Loss 0.0181, Accuracy 1.0000\n",
      "Epoch 6, Train Batch 1, Loss 0.0181, Accuracy 0.9989\n",
      "Epoch 6, Train Batch 2, Loss 0.0210, Accuracy 0.9972\n",
      "Epoch 6, Train Batch 3, Loss 0.0225, Accuracy 0.9973\n",
      "Epoch 6, Train Batch 4, Loss 0.0214, Accuracy 0.9978\n",
      "Epoch 6, Train Batch 5, Loss 0.0216, Accuracy 0.9975\n",
      "Epoch 6, Train Batch 6, Loss 0.0219, Accuracy 0.9973\n",
      "Epoch 6, Train Batch 7, Loss 0.0213, Accuracy 0.9975\n",
      "Epoch 6, Train Batch 8, Loss 0.0213, Accuracy 0.9976\n",
      "Epoch 6, Train Batch 9, Loss 0.0217, Accuracy 0.9976\n",
      "Epoch 6, Train Batch 10, Loss 0.0223, Accuracy 0.9974\n",
      "Epoch 6, Train Batch 11, Loss 0.0232, Accuracy 0.9973\n",
      "Epoch 6, Train Batch 12, Loss 0.0234, Accuracy 0.9971\n",
      "Epoch 6, Train Batch 13, Loss 0.0233, Accuracy 0.9969\n",
      "Epoch 6, Train Batch 14, Loss 0.0231, Accuracy 0.9969\n",
      "Epoch 6, Train Batch 15, Loss 0.0235, Accuracy 0.9967\n",
      "Epoch 6, Train Batch 16, Loss 0.0235, Accuracy 0.9968\n",
      "Epoch 6, Train Batch 17, Loss 0.0230, Accuracy 0.9969\n",
      "Epoch 6, Train Batch 18, Loss 0.0234, Accuracy 0.9968\n",
      "Epoch 6, Train Batch 19, Loss 0.0237, Accuracy 0.9967\n",
      "Epoch 6, Train Batch 20, Loss 0.0242, Accuracy 0.9964\n",
      "Epoch 6, Train Batch 21, Loss 0.0239, Accuracy 0.9965\n",
      "Epoch 6, Train Batch 22, Loss 0.0238, Accuracy 0.9965\n",
      "Epoch 6, Train Batch 23, Loss 0.0240, Accuracy 0.9963\n",
      "Epoch 6, Train Batch 24, Loss 0.0241, Accuracy 0.9962\n",
      "Epoch 6, Train Batch 25, Loss 0.0241, Accuracy 0.9962\n",
      "Epoch 6, Train Batch 26, Loss 0.0238, Accuracy 0.9962\n",
      "Epoch 6, Train Batch 27, Loss 0.0238, Accuracy 0.9963\n",
      "Epoch 6, Train Batch 28, Loss 0.0236, Accuracy 0.9963\n",
      "Epoch 6, Train Batch 29, Loss 0.0236, Accuracy 0.9963\n",
      "Epoch 6, Train Batch 30, Loss 0.0238, Accuracy 0.9962\n",
      "Epoch 6, Train Batch 31, Loss 0.0237, Accuracy 0.9963\n",
      "Epoch 6, Train Batch 32, Loss 0.0240, Accuracy 0.9962\n",
      "Epoch 6, Train Batch 33, Loss 0.0244, Accuracy 0.9960\n",
      "Epoch 6, Train Batch 34, Loss 0.0242, Accuracy 0.9959\n",
      "Epoch 6, Valid Loss 0.5978, Valid Accuracy 0.8273\n",
      "Epoch 7, Train Batch 0, Loss 0.0236, Accuracy 0.9910\n",
      "Epoch 7, Train Batch 1, Loss 0.0223, Accuracy 0.9934\n",
      "Epoch 7, Train Batch 2, Loss 0.0196, Accuracy 0.9950\n",
      "Epoch 7, Train Batch 3, Loss 0.0187, Accuracy 0.9961\n",
      "Epoch 7, Train Batch 4, Loss 0.0180, Accuracy 0.9964\n",
      "Epoch 7, Train Batch 5, Loss 0.0210, Accuracy 0.9955\n",
      "Epoch 7, Train Batch 6, Loss 0.0226, Accuracy 0.9957\n",
      "Epoch 7, Train Batch 7, Loss 0.0226, Accuracy 0.9957\n",
      "Epoch 7, Train Batch 8, Loss 0.0232, Accuracy 0.9957\n",
      "Epoch 7, Train Batch 9, Loss 0.0227, Accuracy 0.9958\n",
      "Epoch 7, Train Batch 10, Loss 0.0227, Accuracy 0.9960\n",
      "Epoch 7, Train Batch 11, Loss 0.0223, Accuracy 0.9962\n",
      "Epoch 7, Train Batch 12, Loss 0.0219, Accuracy 0.9964\n",
      "Epoch 7, Train Batch 13, Loss 0.0212, Accuracy 0.9965\n",
      "Epoch 7, Train Batch 14, Loss 0.0216, Accuracy 0.9965\n",
      "Epoch 7, Train Batch 15, Loss 0.0217, Accuracy 0.9966\n",
      "Epoch 7, Train Batch 16, Loss 0.0218, Accuracy 0.9967\n",
      "Epoch 7, Train Batch 17, Loss 0.0214, Accuracy 0.9968\n",
      "Epoch 7, Train Batch 18, Loss 0.0210, Accuracy 0.9970\n",
      "Epoch 7, Train Batch 19, Loss 0.0210, Accuracy 0.9970\n",
      "Epoch 7, Train Batch 20, Loss 0.0210, Accuracy 0.9969\n",
      "Epoch 7, Train Batch 21, Loss 0.0207, Accuracy 0.9970\n",
      "Epoch 7, Train Batch 22, Loss 0.0206, Accuracy 0.9969\n",
      "Epoch 7, Train Batch 23, Loss 0.0207, Accuracy 0.9969\n",
      "Epoch 7, Train Batch 24, Loss 0.0205, Accuracy 0.9969\n",
      "Epoch 7, Train Batch 25, Loss 0.0204, Accuracy 0.9970\n",
      "Epoch 7, Train Batch 26, Loss 0.0201, Accuracy 0.9971\n",
      "Epoch 7, Train Batch 27, Loss 0.0201, Accuracy 0.9971\n",
      "Epoch 7, Train Batch 28, Loss 0.0199, Accuracy 0.9971\n",
      "Epoch 7, Train Batch 29, Loss 0.0198, Accuracy 0.9971\n",
      "Epoch 7, Train Batch 30, Loss 0.0198, Accuracy 0.9970\n",
      "Epoch 7, Train Batch 31, Loss 0.0195, Accuracy 0.9971\n",
      "Epoch 7, Train Batch 32, Loss 0.0194, Accuracy 0.9971\n",
      "Epoch 7, Train Batch 33, Loss 0.0195, Accuracy 0.9971\n",
      "Epoch 7, Train Batch 34, Loss 0.0195, Accuracy 0.9971\n",
      "Epoch 7, Valid Loss 0.5827, Valid Accuracy 0.8482\n",
      "Epoch 8, Train Batch 0, Loss 0.0125, Accuracy 0.9985\n",
      "Epoch 8, Train Batch 1, Loss 0.0123, Accuracy 0.9993\n",
      "Epoch 8, Train Batch 2, Loss 0.0138, Accuracy 0.9980\n",
      "Epoch 8, Train Batch 3, Loss 0.0124, Accuracy 0.9986\n",
      "Epoch 8, Train Batch 4, Loss 0.0132, Accuracy 0.9989\n",
      "Epoch 8, Train Batch 5, Loss 0.0141, Accuracy 0.9986\n",
      "Epoch 8, Train Batch 6, Loss 0.0141, Accuracy 0.9988\n",
      "Epoch 8, Train Batch 7, Loss 0.0137, Accuracy 0.9988\n",
      "Epoch 8, Train Batch 8, Loss 0.0138, Accuracy 0.9989\n",
      "Epoch 8, Train Batch 9, Loss 0.0143, Accuracy 0.9988\n",
      "Epoch 8, Train Batch 10, Loss 0.0143, Accuracy 0.9989\n",
      "Epoch 8, Train Batch 11, Loss 0.0149, Accuracy 0.9987\n",
      "Epoch 8, Train Batch 12, Loss 0.0156, Accuracy 0.9985\n",
      "Epoch 8, Train Batch 13, Loss 0.0159, Accuracy 0.9985\n",
      "Epoch 8, Train Batch 14, Loss 0.0160, Accuracy 0.9986\n",
      "Epoch 8, Train Batch 15, Loss 0.0158, Accuracy 0.9986\n",
      "Epoch 8, Train Batch 16, Loss 0.0157, Accuracy 0.9987\n",
      "Epoch 8, Train Batch 17, Loss 0.0162, Accuracy 0.9985\n",
      "Epoch 8, Train Batch 18, Loss 0.0167, Accuracy 0.9982\n",
      "Epoch 8, Train Batch 19, Loss 0.0166, Accuracy 0.9983\n",
      "Epoch 8, Train Batch 20, Loss 0.0165, Accuracy 0.9983\n",
      "Epoch 8, Train Batch 21, Loss 0.0170, Accuracy 0.9981\n",
      "Epoch 8, Train Batch 22, Loss 0.0171, Accuracy 0.9980\n",
      "Epoch 8, Train Batch 23, Loss 0.0171, Accuracy 0.9980\n",
      "Epoch 8, Train Batch 24, Loss 0.0171, Accuracy 0.9980\n",
      "Epoch 8, Train Batch 25, Loss 0.0171, Accuracy 0.9981\n",
      "Epoch 8, Train Batch 26, Loss 0.0172, Accuracy 0.9980\n",
      "Epoch 8, Train Batch 27, Loss 0.0174, Accuracy 0.9980\n",
      "Epoch 8, Train Batch 28, Loss 0.0175, Accuracy 0.9980\n",
      "Epoch 8, Train Batch 29, Loss 0.0174, Accuracy 0.9981\n",
      "Epoch 8, Train Batch 30, Loss 0.0174, Accuracy 0.9982\n",
      "Epoch 8, Train Batch 31, Loss 0.0175, Accuracy 0.9981\n",
      "Epoch 8, Train Batch 32, Loss 0.0176, Accuracy 0.9980\n",
      "Epoch 8, Train Batch 33, Loss 0.0176, Accuracy 0.9980\n",
      "Epoch 8, Train Batch 34, Loss 0.0178, Accuracy 0.9979\n",
      "Epoch 8, Valid Loss 0.5475, Valid Accuracy 0.8481\n",
      "Epoch 9, Train Batch 0, Loss 0.0185, Accuracy 0.9989\n",
      "Epoch 9, Train Batch 1, Loss 0.0171, Accuracy 0.9994\n",
      "Epoch 9, Train Batch 2, Loss 0.0159, Accuracy 0.9996\n",
      "Epoch 9, Train Batch 3, Loss 0.0152, Accuracy 0.9993\n",
      "Epoch 9, Train Batch 4, Loss 0.0147, Accuracy 0.9995\n",
      "Epoch 9, Train Batch 5, Loss 0.0153, Accuracy 0.9992\n",
      "Epoch 9, Train Batch 6, Loss 0.0158, Accuracy 0.9988\n",
      "Epoch 9, Train Batch 7, Loss 0.0159, Accuracy 0.9985\n",
      "Epoch 9, Train Batch 8, Loss 0.0164, Accuracy 0.9982\n",
      "Epoch 9, Train Batch 9, Loss 0.0164, Accuracy 0.9981\n",
      "Epoch 9, Train Batch 10, Loss 0.0162, Accuracy 0.9983\n",
      "Epoch 9, Train Batch 11, Loss 0.0166, Accuracy 0.9981\n",
      "Epoch 9, Train Batch 12, Loss 0.0171, Accuracy 0.9976\n",
      "Epoch 9, Train Batch 13, Loss 0.0180, Accuracy 0.9972\n",
      "Epoch 9, Train Batch 14, Loss 0.0190, Accuracy 0.9970\n",
      "Epoch 9, Train Batch 15, Loss 0.0194, Accuracy 0.9970\n",
      "Epoch 9, Train Batch 16, Loss 0.0201, Accuracy 0.9968\n",
      "Epoch 9, Train Batch 17, Loss 0.0202, Accuracy 0.9969\n",
      "Epoch 9, Train Batch 18, Loss 0.0199, Accuracy 0.9971\n",
      "Epoch 9, Train Batch 19, Loss 0.0199, Accuracy 0.9971\n",
      "Epoch 9, Train Batch 20, Loss 0.0202, Accuracy 0.9969\n",
      "Epoch 9, Train Batch 21, Loss 0.0206, Accuracy 0.9970\n",
      "Epoch 9, Train Batch 22, Loss 0.0211, Accuracy 0.9969\n",
      "Epoch 9, Train Batch 23, Loss 0.0209, Accuracy 0.9970\n",
      "Epoch 9, Train Batch 24, Loss 0.0210, Accuracy 0.9971\n",
      "Epoch 9, Train Batch 25, Loss 0.0209, Accuracy 0.9970\n",
      "Epoch 9, Train Batch 26, Loss 0.0214, Accuracy 0.9969\n",
      "Epoch 9, Train Batch 27, Loss 0.0212, Accuracy 0.9969\n",
      "Epoch 9, Train Batch 28, Loss 0.0211, Accuracy 0.9970\n",
      "Epoch 9, Train Batch 29, Loss 0.0213, Accuracy 0.9970\n",
      "Epoch 9, Train Batch 30, Loss 0.0212, Accuracy 0.9970\n",
      "Epoch 9, Train Batch 31, Loss 0.0217, Accuracy 0.9970\n",
      "Epoch 9, Train Batch 32, Loss 0.0217, Accuracy 0.9969\n",
      "Epoch 9, Train Batch 33, Loss 0.0220, Accuracy 0.9968\n",
      "Epoch 9, Train Batch 34, Loss 0.0221, Accuracy 0.9968\n",
      "Epoch 9, Valid Loss 0.5633, Valid Accuracy 0.8455\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Test the model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "source": [
    "model.eval()\n",
    "acc = Accuracy()\n",
    "for batch in test_loader:\n",
    "    batch.to(device)\n",
    "    with torch.no_grad():\n",
    "        pred = model(batch, batch.ndata[\"feat\"]).argmax(dim=1)\n",
    "        acc.update(pred[batch.ndata[\"test_mask\"]], batch.ndata[\"label\"][batch.ndata[\"test_mask\"]])\n",
    "print(\"Accuracy: {:.4f}\".format(acc.value))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Accuracy: 0.8342\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.7",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.7 64-bit ('dgltest': conda)"
  },
  "interpreter": {
   "hash": "8f66b71ca8719224ea73d64d6de76d0f6144fbc22674b30b02bd18d44d337481"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}